ğŸš• End-to-End Data Engineering Pipeline

GCP | PySpark | Kafka | Spark Structured Streaming | Airflow | BigQuery

This project demonstrates a production-grade data engineering pipeline on Google Cloud Platform, supporting batch ETL and real-time streaming use cases.

ğŸ“Š Records processed: 2.7M+
âš™ï¸ Architecture: Batch + Streaming
ğŸ¥ YouTube Demo: https://youtu.be/Cb2BpFoL30g

ğŸ“ˆ Dashboard: https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

ğŸ—ï¸ Architecture
Batch ETL

Raw CSV â†’ GCS â†’ PySpark (Dataproc) â†’ GCS Processed â†’ BigQuery â†’ Looker

Streaming

Kafka â†’ Spark Structured Streaming â†’ BigQuery

ğŸ“ Project Structure
gcp-data-pipeline-project/
â”‚â”€â”€ dags/
â”‚   â””â”€â”€ gcp_etl_dag.py
â”‚â”€â”€ week3/
â”‚   â””â”€â”€ pyspark_etl.py
â”‚â”€â”€ week5-streaming/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â””â”€â”€ spark_streaming.py
â”‚â”€â”€ screenshots/
â”‚â”€â”€ README.md

ğŸš€ Batch ETL Pipeline
1ï¸âƒ£ Upload Raw Data to GCS
gsutil cp big_dataset.csv gs://my-gcp-bucket/raw/big_dataset.csv


2ï¸âƒ£ PySpark Transformation (Dataproc)

week3/pyspark_etl.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp

spark = SparkSession.builder.appName("TaxiETL").getOrCreate()

input_path = "gs://my-gcp-bucket/raw/big_dataset.csv"
output_path = "gs://my-gcp-bucket/processed/taxi_data"

df = spark.read.option("header", True).csv(input_path)

clean_df = (
    df
    .dropna(subset=["pickup_datetime", "fare_amount"])
    .withColumn("pickup_datetime",
                to_timestamp(col("pickup_datetime"), "yyyy-MM-dd HH:mm:ss"))
    .withColumn("fare_amount", col("fare_amount").cast("double"))
)

clean_df.write.mode("overwrite").parquet(output_path)

spark.stop()





3ï¸âƒ£ Load into BigQuery (Airflow)

dags/gcp_etl_dag.py

from airflow import DAG
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.utils.dates import days_ago

with DAG(
    dag_id="gcp_batch_etl",
    start_date=days_ago(1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    load_to_bq = GCSToBigQueryOperator(
        task_id="load_to_bigquery",
        bucket="my-gcp-bucket",
        source_objects=["processed/taxi_data/*"],
        destination_project_dataset_table="my_project.analytics.taxi_trips",
        source_format="PARQUET",
        write_disposition="WRITE_TRUNCATE",
        autodetect=True
    )


4ï¸âƒ£ BigQuery Analytics View
CREATE OR REPLACE VIEW analytics.daily_revenue AS
SELECT
  DATE(pickup_datetime) AS trip_date,
  SUM(fare_amount) AS total_revenue
FROM analytics.taxi_trips
GROUP BY trip_date;





âš¡ Real-Time Streaming Pipeline
5ï¸âƒ£ Kafka Producer

week5-streaming/kafka_producer.py

from kafka import KafkaProducer
import json
import time
import random

producer = KafkaProducer(
    bootstrap_servers="localhost:9092",
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

while True:
    event = {
        "pickup_datetime": time.strftime("%Y-%m-%d %H:%M:%S"),
        "fare_amount": round(random.uniform(5, 50), 2)
    }
    producer.send("taxi_topic", event)
    time.sleep(1)

6ï¸âƒ£ Spark Structured Streaming Consumer

week5-streaming/spark_streaming.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, DoubleType

spark = SparkSession.builder.appName("KafkaStreaming").getOrCreate()

schema = StructType() \
    .add("pickup_datetime", StringType()) \
    .add("fare_amount", DoubleType())

kafka_df = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "taxi_topic")
    .load()
)

parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

query = (
    parsed_df.writeStream
    .format("console")
    .outputMode("append")
    .start()
)

query.awaitTermination()


ğŸ“Š BI Dashboard (Looker Studio)

Metrics:

Daily revenue

Peak hours

Trip density

Pickup zones

ğŸ§° Tech Stack
Layer	Tools
Cloud	GCP (GCS, Dataproc, BigQuery, Composer)
Processing	PySpark, Spark Structured Streaming
Streaming	Apache Kafka
Orchestration	Airflow
BI	Looker Studio
Language	Python, SQL
ğŸ“ˆ Key Outcomes

Built batch + streaming hybrid architecture

Processed 2.7M+ records

Automated pipelines using Airflow

Delivered analytics-ready datasets

Implemented fault-tolerant streaming

ğŸ“¬ Contact

Ruthvik Kumar Yadav Maram
ğŸ“ Schaumburg, IL
ğŸ“§ ruthvikyadav930@gmail.com

ğŸ”— LinkedIn: https://www.linkedin.com/in/ruthvikyadav/

ğŸ”— GitHub: https://github.com/Ruthvikyadavm

âœ… IMPORTANT

Make sure these files exist:

screenshots/
â”œâ”€â”€ architecture-batch.png
â”œâ”€â”€ architecture-streaming.png
â”œâ”€â”€ gcs-raw.png
â”œâ”€â”€ dataproc-create.png
â”œâ”€â”€ gcs-processed.png
â”œâ”€â”€ bq-table-schema.png
â”œâ”€â”€ bq-views.png
â”œâ”€â”€ airflow-run.png
â”œâ”€â”€ spark-stream.png
â”œâ”€â”€ dashboard.png
