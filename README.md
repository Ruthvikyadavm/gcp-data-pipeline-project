ðŸš€ End-to-End Data Engineering Pipeline on Google Cloud Platform (GCP)

This project showcases a production-grade data pipeline built using
Google Cloud Storage, PySpark on Dataproc, BigQuery, Apache Airflow, and Looker Studio.

The pipeline ingests raw NYC Yellow Taxi data â†’ transforms it using distributed PySpark â†’ loads it into BigQuery â†’ visualizes insights â†’ and finally supports real-time streaming via Kafka + Spark Structured Streaming.

âš¡ TL;DR â€” What I Built

âœ” Batch ingestion â†’ GCS
âœ” Distributed processing â†’ Dataproc (PySpark)
âœ” Data warehouse â†’ BigQuery
âœ” Visualization â†’ Looker Studio
âœ” Orchestration â†’ Airflow DAG
âœ” Real-time streaming â†’ Kafka + Spark Structured Streaming
âœ” Processed 2.7M+ records with optimized transformations

ðŸ—ï¸ Full Architecture (Batch Pipeline)
Raw CSV â†’ Google Cloud Storage (Raw Zone)
        â†’ Dataproc PySpark Job (Transform & Clean)
        â†’ Google Cloud Storage (Processed Zone)
        â†’ BigQuery Partitioned Tables (Analytics Warehouse)
        â†’ Looker Studio Reports & Dashboards


ðŸ› ï¸ Technologies Used
Component	Technology
Cloud	Google Cloud Platform
Storage	Google Cloud Storage (GCS)
Compute	Dataproc (PySpark)
ETL	Python, Spark
Warehouse	BigQuery
Orchestration	Apache Airflow
Visualization	Looker Studio
Streaming	Kafka + Spark Structured Streaming
Language	Python, SQL
Version Control	Git & GitHub


ðŸ“ Repository Structure
gcp-data-pipeline-project/
â”‚
â”œâ”€â”€ week1/        # Raw ingestion to GCS + BigQuery load
â”œâ”€â”€ week2/        # Data cleaning + automated pipeline
â”œâ”€â”€ week3/        # PySpark on Dataproc + processed outputs
â”œâ”€â”€ week4/        # BigQuery views + Looker dashboard
â”œâ”€â”€ week5/        # Kafka + Spark Structured Streaming
â”‚
â”œâ”€â”€ dags/         # Airflow DAG files
â”‚   â””â”€â”€ gcs_to_bigquery_dag.py
â”‚
â”œâ”€â”€ screenshots/  # Architecture, GCS, BQ, Dashboard screenshots
â””â”€â”€ README.md

ðŸŽ¯ How to Run the Project (Quick Start)
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Upload Raw CSV to GCS
gsutil cp nyc_taxi_raw.csv gs://ruthvik-week3-bucket-2/raw/

3ï¸âƒ£ Run Week 2 Cleanup
python week2/clean_data.py

4ï¸âƒ£ Submit PySpark Job on Dataproc
gcloud dataproc jobs submit pyspark \
    --cluster=my-spark-cluster \
    --region=us-central1 \
    week3/process_big_data.py

5ï¸âƒ£ Airflow DAG Loads Processed CSV â†’ BigQuery Automatically
6ï¸âƒ£ Open Looker Dashboard (Live Link)

ðŸ”— https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

ðŸ“¦ Week 1 â€“ Data Ingestion & Environment Setup
âœ” Tasks Completed

Created GCP project, service account, IAM roles

Uploaded raw CSV into GCS (Raw Zone)

Loaded raw data into BigQuery using autodetect

Explored dataset using SQL queries

ðŸ§  Skills Practiced

GCS buckets, BigQuery tables, schema detection, gcloud CLI, Python.

ðŸ§¹ Week 2 â€“ Data Cleaning & Automated ETL
âœ” Tasks Completed

Performed data cleaning using Pandas

Fixed inconsistent types, nulls, outliers

Uploaded the cleaned dataset to GCS (Processed Zone)

Automated ingestion script

ðŸ§  Skills Practiced

Python ETL, Pandas, data quality checks, automation.

âš¡ Week 3 â€“ Distributed Processing with PySpark (Dataproc)
âœ” Tasks Completed

Created Dataproc cluster

Executed PySpark job on 2.7M+ records

Stored processed outputs into GCS as Parquet/CSV

Connected BigQuery to processed data

ðŸ§  Skills Practiced

Spark DataFrames, partitioning, cluster-based ETL, optimization.

ðŸ“Š Week 4 â€“ BigQuery Analytics + Looker Dashboard
âœ” Views Created
View	Purpose
trips_by_passenger	Avg fare by passenger count
trips_over_time	Daily trip patterns
high_fare_trips	Outlier fare detection
âœ” Dashboard Includes

Daily revenue trend

Avg distance trend

Top pickup zones

Fare distribution

ðŸ”— Live Dashboard:
https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

ðŸŒ¬ï¸ Week 5 â€“ Real-Time Streaming (Kafka + Spark Structured Streaming)

This week adds near real-time micro-batch processing.

ðŸ”¥ Architecture
Producer â†’ Kafka Topic (`taxi_trips`) 
        â†’ Spark Structured Streaming (JSON processing)
        â†’ GCS / Local JSON Output

âœ” Technologies Used

Kafka 3.5.1, Zookeeper, Spark 3.5.1, JSON events, WSL2 Ubuntu.

âœ” Real Output Sample
{"VendorID":1,"trip_distance":3.5,"fare_amount":12.5,...}

âœ” What I Learned

Kafka topic creation

Producer/Consumer basics

Micro-batch streaming

Checkpointing & fault tolerance

Fixing Spark classpath issues

ðŸŒ€ Orchestration â€” Airflow DAG

This DAG automatically:

Lists all processed CSV files in GCS

Loads them into BigQuery

Truncates table and refreshes analytics daily

ðŸ“Œ DAG Code Used
from datetime import datetime
from airflow import DAG
from airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator


with DAG(
    dag_id="gcs_to_bigquery_dag",
    start_date=datetime(2025, 11, 8),
    schedule="0 0 * * *",   # Runs once per day
    catchup=False,
    tags=["gcs", "bigquery"],
):
    
    list_gcs_files = GCSListObjectsOperator(
        task_id="list_gcs_files",
        bucket="ruthvik-week3-bucket-2",
        prefix="",
        gcp_conn_id="google_cloud_default"
    )

    load_to_bigquery = GCSToBigQueryOperator(
        task_id="load_to_bigquery",
        bucket="ruthvik-week3-bucket-2",
        source_objects=["week3/output/part-*.csv"],
        destination_project_dataset_table="ruthvik-week3-dataproc.nyc_taxi_demo.trips",
        write_disposition="WRITE_TRUNCATE",
        source_format="CSV",
        skip_leading_rows=1,
        autodetect=True,
        gcp_conn_id="google_cloud_default",
    )

    list_gcs_files >> load_to_bigquery

âœ” Add this Screenshot

Airflow Graph view

Airflow Code view

Successful DAG run

ðŸ“ˆ Key Insights from the Data

2.8M+ rides analyzed

Avg fare stays stable for 1â€“4 passengers, spikes at 7+

Trip volume spikes â‰  fare spikes

Pickup hotspots: Manhattan transit, tourist zones

Seasonal patterns in trip count & revenue

ðŸŽ¯ Next Steps (Planned Enhancements)

Stream Kafka data directly into BigQuery

Add Kafka Connect + Schema Registry

Add Terraform (IaC for bucket, cluster, BQ)

Add CI/CD using Cloud Build or GitHub Actions

Build a monitoring dashboard (Cloud Logging + Grafana)

ðŸ”— Important Links

ðŸ”¸ Dashboard
https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

ðŸ”¸ LinkedIn
https://www.linkedin.com/in/ruthvikyadav/

ðŸ”¸ GitHub Repository
https://github.com/Ruthvikyadavm/gcp-data-pipeline-project
