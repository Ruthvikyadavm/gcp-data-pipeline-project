ðŸš€ End-to-End Data Engineering Pipeline on Google Cloud Platform (GCP)

This project showcases a production-grade data pipeline built using
Google Cloud Storage, PySpark on Dataproc, BigQuery, Apache Airflow, Looker Studio, and Kafka Streaming.

The pipeline ingests raw NYC Yellow Taxi data â†’ processes it using distributed PySpark â†’ loads it into BigQuery â†’ visualizes insights â†’ and supports real-time streaming via Kafka + Spark Structured Streaming.

-----

âš¡ TL;DR â€” What I Built

ðŸŸ¡ Raw â†’ GCS (Raw Zone)

ðŸ”µ Transform â†’ PySpark on Dataproc

ðŸŸ£ Store â†’ BigQuery Analytics Warehouse

ðŸŸ¢ Visualize â†’ Looker Studio Dashboard

ðŸ”´ Orchestrate â†’ Apache Airflow DAG

ðŸŸ  Stream â†’ Kafka + Spark Structured Streaming

âš¡ Processed 2.7M+ taxi trip records end-to-end

-----

ðŸ—ï¸ Architecture (Batch Pipeline)
Raw CSV 
   â†’ Google Cloud Storage (Raw Zone)
   â†’ Dataproc PySpark Job (Transform & Clean)
   â†’ Google Cloud Storage (Processed Zone)
   â†’ BigQuery (Partitioned Analytics Warehouse)
   â†’ Looker Studio (Visualization)


---


ðŸ› ï¸ Technologies Used
Component	                Technology
Cloud	                        Google Cloud Platform
Storage	                        Google Cloud Storage (GCS)
Compute	                        Dataproc (PySpark)
ETL	                        Python, Spark
Warehouse	                BigQuery
Orchestration	                Apache Airflow
Visualization	                Looker Studio
Streaming	                Kafka + Spark Structured Streaming
Language	                Python, SQL
Version Control	                Git & GitHub

----


ðŸ“ Repository Structure
gcp-data-pipeline-project/
â”‚
â”œâ”€â”€ week1/        # Raw ingestion to GCS + BigQuery load
â”œâ”€â”€ week2/        # Data cleaning + automated pipeline
â”œâ”€â”€ week3/        # PySpark on Dataproc + processed outputs
â”œâ”€â”€ week4/        # BigQuery views + Looker dashboard
â”œâ”€â”€ week5/        # Kafka + Spark Structured Streaming
â”‚
â”œâ”€â”€ dags/         # Airflow DAG files
â”‚   â””â”€â”€ gcs_to_bigquery_dag.py
â”‚
â”œâ”€â”€ screenshots/  # Architecture, GCS, BQ, Dashboard screenshots
â””â”€â”€ README.md

-----

ðŸŽ¯ How to Run the Project (Quick Start)

1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Upload Raw CSV to GCS
gsutil cp nyc_taxi_raw.csv gs://ruthvik-week3-bucket-2/raw/

3ï¸âƒ£ Run Week 2 Cleanup
python week2/clean_data.py

4ï¸âƒ£ Submit PySpark Job on Dataproc
gcloud dataproc jobs submit pyspark \
    --cluster=my-spark-cluster \
    --region=us-central1 \
    week3/process_big_data.py

5ï¸âƒ£ Airflow DAG Loads Processed CSV â†’ BigQuery Automatically

6ï¸âƒ£ Open Looker Dashboard (Live Link)

ðŸ”— https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

---------

ðŸ“¦ Week 1 â€“ Data Ingestion & Environment Setup

âœ” Tasks Completed

Created GCP project, service account, IAM roles

Uploaded raw CSV into GCS (Raw Zone)

Loaded raw data into BigQuery using autodetect

Explored dataset using SQL queries

ðŸ§  Skills Practiced

GCS buckets, BigQuery tables, schema detection, gcloud CLI, Python.

-----

ðŸ§¹ Week 2 â€“ Data Cleaning & Automated ETL

âœ” Tasks Completed

Performed data cleaning using Pandas

Fixed inconsistent types, nulls, outliers

Uploaded the cleaned dataset to GCS (Processed Zone)

Automated ingestion script

ðŸ§  Skills Practiced

Python ETL, Pandas, data quality checks, automation.

---

âš¡ Week 3 â€“ Distributed Processing with PySpark (Dataproc)

âœ” Tasks Completed

Created Dataproc cluster

Executed PySpark job on 2.7M+ records

Stored processed outputs into GCS as Parquet/CSV

Connected BigQuery to processed data

ðŸ§  Skills Practiced

Spark DataFrames, partitioning, cluster-based ETL, optimization.

---

ðŸ“Š Week 4 â€” BigQuery Analytics + Looker Studio Dashboard

âœ” SQL Views Created

View	                    Description
trips_by_passenger	     Avg fare vs passenger count
trips_over_time	        Daily trip counts and seasonality
high_fare_trips	        Outlier/high value rides

-----

âœ” Dashboard Includes

Daily revenue trend

Avg distance trend

Top pickup zones

Fare distribution

ðŸ”— Live Dashboard:
https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

-------

ðŸŒ¬ï¸ Week 5 â€“ Real-Time Streaming (Kafka + Spark Structured Streaming)

This week adds near real-time micro-batch processing.

ðŸ”¥ Architecture
Producer â†’ Kafka Topic (`taxi_trips`) 
        â†’ Spark Structured Streaming (JSON processing)
        â†’ GCS / Local JSON Output

âœ” Technologies Used

Kafka 3.5.1, Zookeeper, Spark 3.5.1, JSON events, WSL2 Ubuntu.

âœ” Real Output Sample
{"VendorID":1,"trip_distance":3.5,"fare_amount":12.5,...}

âœ” What I Learned

Kafka topic creation

Producer/Consumer basics

Micro-batch streaming

Checkpointing & fault tolerance

Fixing Spark classpath issues

-------

ðŸŒ€ Orchestration â€” Airflow DAG

This DAG automatically:

Lists all processed CSV files in GCS

Loads them into BigQuery

Truncates table and refreshes analytics daily

----

ðŸ“Œ DAG Code Used

from datetime import datetime
from airflow import DAG
from airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator


with DAG(
    dag_id="gcs_to_bigquery_dag",
    start_date=datetime(2025, 11, 8),
    schedule="0 0 * * *",   # Runs once per day
    catchup=False,
    tags=["gcs", "bigquery"],
):
    
    list_gcs_files = GCSListObjectsOperator(
        task_id="list_gcs_files",
        bucket="ruthvik-week3-bucket-2",
        prefix="",
        gcp_conn_id="google_cloud_default"
    )

    load_to_bigquery = GCSToBigQueryOperator(
        task_id="load_to_bigquery",
        bucket="ruthvik-week3-bucket-2",
        source_objects=["week3/output/part-*.csv"],
        destination_project_dataset_table="ruthvik-week3-dataproc.nyc_taxi_demo.trips",
        write_disposition="WRITE_TRUNCATE",
        source_format="CSV",
        skip_leading_rows=1,
        autodetect=True,
        gcp_conn_id="google_cloud_default",
    )

    list_gcs_files >> load_to_bigquery

-----


Airflow Graph view

Airflow Code view

Successful DAG run

------

ðŸ“ˆ Key Insights from the Data

2.8M+ rides analyzed

Avg fare stays stable for 1â€“4 passengers, spikes at 7+

Trip volume spikes â‰  fare spikes

Pickup hotspots: Manhattan transit, tourist zones

Seasonal patterns in trip count & revenue

----

ðŸŽ¯ Next Steps (Planned Enhancements)

Stream Kafka data directly into BigQuery

Add Kafka Connect + Schema Registry

Add Terraform (IaC for bucket, cluster, BQ)

Add CI/CD using Cloud Build or GitHub Actions

Build a monitoring dashboard (Cloud Logging + Grafana)

----

ðŸ”— Important Links

ðŸ”¸ Dashboard
https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b

ðŸ”¸ LinkedIn
https://www.linkedin.com/in/ruthvikyadav/

ðŸ”¸ GitHub Repository
https://github.com/Ruthvikyadavm/gcp-data-pipeline-project


