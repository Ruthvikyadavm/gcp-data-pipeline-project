# ğŸš• End-to-End Data Engineering Pipeline (GCP + Kafka + PySpark + Airflow)

This project demonstrates a **production-grade data pipeline** built using  
**Google Cloud Platform, Apache Kafka, PySpark on Dataproc, Airflow, and BigQuery**,  
including both **batch processing** and **real-time streaming**.

ğŸ”— **YouTube Walkthrough:** https://youtu.be/Cb2BpFoL30g  
ğŸ”— **Looker Studio Dashboard:** https://lookerstudio.google.com/reporting/9d456692-cd86-460e-9bbd-58e1bdc4413b  

---

## ğŸ“Œ Architecture Overview

### **Batch Pipeline (ETL)**
Raw CSV â†’ GCS Raw Zone â†’ PySpark on Dataproc â†’ GCS Processed Zone â†’ BigQuery â†’ Dashboard

![architecture-batch](screenshots/batch_architecture.png)

### **Real-Time Pipeline**
Kafka Producer â†’ Kafka Topic â†’ Spark Structured Streaming â†’ BigQuery Live Table

![architecture-streaming](screenshots/stream_architecture.png)

---

# ğŸš€ **1. Batch ETL Pipeline (GCP)**

### **1.1 Upload raw dataset to GCS**

- Stored raw CSV in: `gs://<bucket>/week3/big_dataset.csv`

ğŸ“¸ Screenshot:  
![gcs-raw](screenshots/gcs_raw.png)

---

### **1.2 Dataproc PySpark Transformation**

PySpark cleans & transforms 2.7M+ rows:

- Schema enforcement  
- Null handling  
- Datetime conversion  
- Partitioning  
- Writes processed output to GCS  

ğŸ“¸ Dataproc Cluster Creation:  
![dataproc-create](screenshots/dataproc_cluster (2).png)

ğŸ“¸ PySpark Output in GCS:  
![gcs-processed](screenshots/dataproc_cluster.png)

---

### **1.3 Load to BigQuery**

Processed files loaded to BigQuery using:

- GCSToBigQueryOperator (Airflow)  
- Partitioned tables  
- Analytical views  

ğŸ“¸ BigQuery Table Preview:  
![bq-table-schema](screenshots/bigquery_tables.png)

ğŸ“¸ BigQuery Views:  
![bq-views](screenshots/views.png)

---

# âš™ï¸ **2. Airflow Orchestration**

The full pipeline is automated with Airflow:

- List files in GCS  
- Run PySpark job (optional)  
- Load processed data into BigQuery  
- Generate views  

ğŸ“¸ Airflow DAG Run:  
![airflow-run](screenshots/airflowui.png)

---

# âš¡ **3. Real-Time Streaming (Kafka + Spark Structured Streaming)**

### **3.1 Kafka Setup**

- Created Kafka topics  
- Producer sends taxi trip events  
- Consumer verifies stream  

### **3.2 Spark Structured Streaming Job**

- Reads Kafka JSON messages  
- Normalizes timestamps  
- Writes micro-batches to output sink  
- Fault-tolerant with checkpointing  

ğŸ“¸ Spark Streaming Log (Optional screenshot):  
![spark-stream](screenshots/spark.png)

---

# ğŸ“Š **4. Looker Studio Dashboard**

Interactive dashboard includes:

- Revenue trends  
- Trip density  
- Peak hours  
- Popular pickup zones  

ğŸ“¸ Dashboard screenshot placeholder  
![dashboard](screenshots/looker_dashboard.png)

---

# ğŸ§° **Tech Stack**

| Category | Tools |
|---------|-------|
| Cloud | GCP (GCS, Dataproc, BigQuery, Composer) |
| Processing | PySpark, Spark Structured Streaming |
| Streaming | Apache Kafka |
| Orchestration | Airflow |
| Dashboard | Looker Studio |
| Language | Python, SQL |

---

# ğŸ“ˆ Key Achievements

- Processed **2.7M+ taxi records** end-to-end  
- Automated entire pipeline with Airflow  
- Built **real-time + batch** hybrid architecture  
- Created reusable architecture used in real data engineering workflows  
- Developed a public YouTube walkthrough  

---

# ğŸ“ Repository Structure

gcp-data-pipeline-project/
â”‚â”€â”€ week1/ # Raw ingestion
â”‚â”€â”€ week2/ # Cleaning & validation
â”‚â”€â”€ week3/ # PySpark + Dataproc job
â”‚â”€â”€ week4/ # BigQuery analytics
â”‚â”€â”€ week5-streaming/ # Kafka + Spark streaming
â”‚â”€â”€ dags/ # Airflow DAGs
â”‚â”€â”€ screenshots/ # Architecture + UI screenshots
â”‚â”€â”€ README.md

---


---

# â–¶ï¸ How to Run This Project

### **Batch Pipeline**
1. Upload dataset to GCS  
2. Spin up Dataproc cluster  
3. Run PySpark job  
4. Load into BigQuery  
5. Use Looker Studio for visualization  

### **Streaming Pipeline**
1. Start Kafka broker & topic  
2. Run producer script  
3. Start Spark Structured Streaming job  
4. Query live data  

---

# ğŸ“¬ Contact

**Ruthvik Kumar Yadav Maram**  
ğŸ“ Schaumburg, IL  
ğŸ“§ ruthvikyadav930@gmail.com  
ğŸ”— LinkedIn: https://www.linkedin.com/in/ruthvikyadav/  
ğŸ”— GitHub: https://github.com/Ruthvikyadavm  
ğŸ”— YouTube Demo: https://youtu.be/Cb2BpFoL30g  

---






